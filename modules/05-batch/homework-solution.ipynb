{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark.__version__: 3.5.1\n",
      "/Users/ronaldfung/Projects/data-engineering-zoomcamp-2024/modules/05-batch\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "WDIR = os.path.abspath(os.path.dirname(\"\"))\n",
    "print(f\"pyspark.__version__: {pyspark.__version__}\")\n",
    "print(WDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {\n",
    "    \"fhv\": types.StructType([\n",
    "        types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "        types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"dropoff_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"pulocationid\", types.IntegerType(), True),\n",
    "        types.StructField(\"dolocationid\", types.IntegerType(), True),\n",
    "        types.StructField(\"sr_flag\", types.StringType(), True),\n",
    "        types.StructField(\"affiliated_base_number\", types.StringType(), True),\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vehicle_type in [\"fhv\"]:\n",
    "    for year in range(2019, 2020): # 2019\n",
    "        for month in range(10, 11): # 10\n",
    "            input_path = os.path.join(WDIR, \"bucket\", \"bronze\", f\"{vehicle_type}\", f\"{year}\", f\"{month:02d}\")\n",
    "            df = spark.read \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .option(\"compression\", \"gzip\") \\\n",
    "                    .schema(schemas[vehicle_type]) \\\n",
    "                    .csv(input_path)\n",
    "\n",
    "            df \\\n",
    "                .repartition(6) \\\n",
    "                .write.parquet(f'bucket/pq/{vehicle_type}/{year}/{month:02d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, lit, unix_timestamp, round, trim, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+--------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|pulocationid|dolocationid|sr_flag|affiliated_base_number|              tripid|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+--------------------+\n",
      "|              B02546|2019-10-04 12:01:47|2019-10-04 12:15:31|         264|         247|   NULL|                B02546|B02546|2019-10-04...|\n",
      "|              B00037|2019-10-15 08:55:16|2019-10-15 09:21:04|         264|         155|   NULL|                B00037|B00037|2019-10-15...|\n",
      "|              B00445|2019-10-07 03:54:41|2019-10-07 04:02:41|         252|         138|   NULL|                B00445|B00445|2019-10-07...|\n",
      "|     B01711         |2019-10-20 14:53:46|2019-10-20 15:07:34|          73|          92|   NULL|       B01711         |B01711|2019-10-20...|\n",
      "|              B03032|2019-10-01 09:39:24|2019-10-01 10:18:58|         264|         231|   NULL|                B03032|B03032|2019-10-01...|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv = spark.read.parquet(\"bucket/pq/fhv/2019/10\")\n",
    "df_fhv = df_fhv.withColumn('tripid', concat(trim(df_fhv.dispatching_base_num), lit('|'), trim(df_fhv.pickup_datetime.cast('string'))))\n",
    "df_fhv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 - Count taxi trips on 15th October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv \\\n",
    "    .where(lit(to_date(df_fhv.pickup_datetime)) == \"2019-10-15\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4 - Length of the longest trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|diff_in_hours|\n",
      "+--------------------+-------------------+-------------------+-------------+\n",
      "|              B02832|2019-10-28 09:00:00|2091-10-28 09:30:00|     631152.5|\n",
      "|              B02832|2019-10-11 18:00:00|2091-10-11 18:30:00|     631152.5|\n",
      "|              B02416|2019-10-31 23:46:33|2029-11-01 00:13:00|     87672.44|\n",
      "|     B00746         |2019-10-01 21:43:42|2027-10-01 21:45:23|     70128.03|\n",
      "|              B02921|2019-10-17 14:00:00|2020-10-18 00:00:00|       8794.0|\n",
      "+--------------------+-------------------+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv \\\n",
    "    .withColumn('diff_in_hours', round((unix_timestamp(\"dropoff_datetime\") - unix_timestamp('pickup_datetime'))/3600, 2)) \\\n",
    "    .orderBy('diff_in_hours', ascending=False) \\\n",
    "    .select(['dispatching_base_num', 'pickup_datetime', 'dropoff_datetime', 'diff_in_hours']) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6 - Least frequent pickup location zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone_data = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(os.path.join(WDIR, \"data\", 'taxi+_zone_lookup.csv'))\n",
    "df_zone_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Zone|count|\n",
      "+--------------------+-----+\n",
      "|         Jamaica Bay|    1|\n",
      "|Governor's Island...|    2|\n",
      "| Green-Wood Cemetery|    5|\n",
      "|       Broad Channel|    8|\n",
      "|     Highbridge Park|   14|\n",
      "|        Battery Park|   15|\n",
      "|Saint Michaels Ce...|   23|\n",
      "|Breezy Point/Fort...|   25|\n",
      "|Marine Park/Floyd...|   26|\n",
      "|        Astoria Park|   29|\n",
      "|    Inwood Hill Park|   39|\n",
      "|       Willets Point|   47|\n",
      "|Forest Park/Highl...|   53|\n",
      "|  Brooklyn Navy Yard|   57|\n",
      "|        Crotona Park|   62|\n",
      "|        Country Club|   77|\n",
      "|     Freshkills Park|   89|\n",
      "|       Prospect Park|   98|\n",
      "|     Columbia Street|  105|\n",
      "|  South Williamsburg|  110|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.select(['tripid', 'pulocationid']) \\\n",
    "    .join(df_zone_data.select(['LocationID', 'Zone']), df_fhv.pulocationid == df_zone_data.LocationID, \"left\") \\\n",
    "    .groupBy('Zone') \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=True) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
